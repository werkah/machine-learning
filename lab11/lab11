#!/usr/bin/env python
# coding: utf-8

# In[1]:


from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
housing=fetch_california_housing()
X_train_full, X_test, y_train_full, y_test=train_test_split(housing.data,housing.target, random_state=42)
X_train, X_valid, y_train, y_valid=train_test_split(X_train_full,y_train_full, random_state=42)
scaler=StandardScaler()
X_train=scaler.fit_transform(X_train)
X_valid=scaler.transform(X_valid)
X_test=scaler.transform(X_test)


# In[2]:


from scipy.stats import reciprocal
import numpy as np
param_distribs={"model__n_hidden": [0, 1, 2, 3],
                "model__n_neurons": np.arange(1, 100),
                "model__learning_rate": reciprocal(3e-4,3e-2).rvs(1000).tolist(),
                "model__optimizer": ['adam', 'sgd', 'nesterov']}


# In[3]:


def build_model(n_hidden, n_neurons, optimizer, learning_rate):
    model=tf.keras.models.Sequential()
    model.add(tf.keras.layers.InputLayer(input_shape=X_train.shape[1:]))
    for _ in range(n_hidden):
        model.add(tf.keras.layers.Dense(n_neurons, activation='relu'))
    if optimizer == "sgd":
        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)
    elif optimizer == "nesterov":
        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, nesterov=True)
    elif optimizer == "adam":
        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    model.add(tf.keras.layers.Dense(1))
    model.compile(loss="mse",optimizer=optimizer)
    return model


# In[4]:


import scikeras
from scikeras.wrappers import KerasRegressor
es=tf.keras.callbacks.EarlyStopping(patience=10, min_delta=1.0, verbose=1)
keras_reg=KerasRegressor(build_model, callbacks=[es])


# In[5]:


from sklearn.model_selection import RandomizedSearchCV
rnd_search_cv=RandomizedSearchCV(keras_reg,param_distribs,n_iter=10,cv=3,verbose=2)
rnd_search_cv.fit(X_train, y_train, epochs=100, validation_data=(X_valid,y_valid), verbose=0)


# In[6]:


best_params = rnd_search_cv.best_params_


# In[7]:


print(best_params)


# In[8]:


import pickle


# In[9]:


with open('rnd_search_params.pkl', 'wb') as file:
    pickle.dump(best_params, file)


# In[10]:


with open('rnd_search_scikeras.pkl', 'wb') as file:
    pickle.dump(rnd_search_cv, file)


# In[11]:


import keras_tuner as kt


# In[13]:


def build_model_kt(hp):
    n_hidden = hp.Int("n_hidden", min_value=0, max_value=3, default=2)
    n_neurons = hp.Int("n_neurons", min_value=1, max_value=100)
    learning_rate = hp.Float("learning_rate", min_value=3e-4, max_value=3e-2, sampling="log")
    optimizer = hp.Choice("optimizer", values=["adam", "sgd", "nesterov"])
    model=tf.keras.models.Sequential()
    model.add(tf.keras.layers.InputLayer(input_shape=X_train.shape[1:]))
    for _ in range(n_hidden):
        model.add(tf.keras.layers.Dense(n_neurons, activation='relu'))
    if optimizer == "sgd":
        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)
    elif optimizer == "nesterov":
        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, nesterov=True)
    elif optimizer == "adam":
        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    model.add(tf.keras.layers.Dense(1))
    model.compile(loss="mse", optimizer=optimizer, metrics=['mse'])
    return model


# In[14]:


random_search_tuner=kt.RandomSearch(build_model_kt, objective="val_loss", max_trials=10, overwrite=True,directory="my_california_housing", project_name="my_rnd_search", seed=42)


# In[15]:


import os
root_logdir=os.path.join(random_search_tuner.project_dir,'tensorboard')
tb=tf.keras.callbacks.TensorBoard(root_logdir)


# In[16]:


es=tf.keras.callbacks.EarlyStopping(patience=10, min_delta=1.0, verbose=1)


# In[17]:


random_search_tuner.search(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[tb, es])


# In[18]:


best_hp = random_search_tuner.get_best_hyperparameters(1)[0].values


# In[19]:


print(best_hp)


# In[20]:


with open('kt_search_params.pkl', 'wb') as file:
    pickle.dump(best_hp, file)


# In[21]:


best_model = random_search_tuner.get_best_models(1)[0]


# In[22]:


best_model.save('kt_best_model.h5')


# In[ ]:



